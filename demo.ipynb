{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff99daaa",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f24a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from inference import SimpleInference\n",
    "\n",
    "# # Initialize model\n",
    "# engine = SimpleInference()\n",
    "\n",
    "# # Generate description\n",
    "# engine.generate(\n",
    "#     image_path=\"examples/city.jpg\",\n",
    "#     prompt=\"describe\",\n",
    "#     max_tokens=1024,\n",
    "#     temperature=0.8,\n",
    "#     top_p = 0.9\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b892a18",
   "metadata": {},
   "source": [
    "# Step 1: Prepare the image and prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e63d07",
   "metadata": {},
   "source": [
    "## Load the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0695da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image = Image.open(\"examples/panda.jpg\")\n",
    "image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627093c3",
   "metadata": {},
   "source": [
    "## Resize the image to 224x224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c40842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Optional, Union, Iterable, Dict\n",
    "\n",
    "def rescale(\n",
    "    image: np.ndarray, scale: float, dtype: np.dtype = np.float32\n",
    ") -> np.ndarray:\n",
    "    rescaled_image = image * scale\n",
    "    rescaled_image = rescaled_image.astype(dtype)\n",
    "    return rescaled_image\n",
    "\n",
    "\n",
    "def resize(\n",
    "    image: Image,\n",
    "    size: Tuple[int, int],\n",
    "    resample: Image.Resampling = None,\n",
    "    reducing_gap: Optional[int] = None,\n",
    ") -> np.ndarray:\n",
    "    height, width = size\n",
    "    resized_image = image.resize(\n",
    "        (width, height), resample=resample, reducing_gap=reducing_gap\n",
    "    )\n",
    "    return resized_image\n",
    "\n",
    "\n",
    "def normalize(\n",
    "    image: np.ndarray,\n",
    "    mean: Union[float, Iterable[float]],\n",
    "    std: Union[float, Iterable[float]],\n",
    ") -> np.ndarray:\n",
    "    mean = np.array(mean, dtype=image.dtype)\n",
    "    std = np.array(std, dtype=image.dtype)\n",
    "    image = (image - mean) / std\n",
    "    return image\n",
    "\n",
    "\n",
    "def process_images(\n",
    "    images: List[Image.Image],\n",
    "    size: Dict[str, int] = None,\n",
    "    resample: Image.Resampling = None,\n",
    "    rescale_factor: float = None,\n",
    "    image_mean: Optional[Union[float, List[float]]] = None,\n",
    "    image_std: Optional[Union[float, List[float]]] = None,\n",
    ") -> List[np.ndarray]:\n",
    "    height, width = size[0], size[1]\n",
    "    images = [\n",
    "        resize(image=image, size=(height, width), resample=resample) for image in images\n",
    "    ]\n",
    "    images = [np.array(image) for image in images]\n",
    "    # Rescale the pixel values to be in the range [0, 1]\n",
    "    images = [rescale(image, scale=rescale_factor) for image in images]\n",
    "    # Normalize the images to have mean 0 and standard deviation 1\n",
    "    images = [normalize(image, mean=image_mean, std=image_std) for image in images]\n",
    "    # Move the channel dimension to the first dimension as the model expects images in the format [Channel, Height, Width]\n",
    "    images = [image.transpose(2, 0, 1) for image in images]\n",
    "\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647045e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "IMAGENET_STANDARD_MEAN = [0.5, 0.5, 0.5]  # From HF code\n",
    "IMAGENET_STANDARD_STD = [0.5, 0.5, 0.5]  # From HF code\n",
    "\n",
    "image = Image.open(\"examples/home.jpg\")\n",
    "\n",
    "pixel_values = process_images(\n",
    "    [image],\n",
    "    size=(224, 224),\n",
    "    resample=Image.Resampling.BICUBIC,\n",
    "    rescale_factor=1 / 255.0,\n",
    "    image_mean=IMAGENET_STANDARD_MEAN,\n",
    "    image_std=IMAGENET_STANDARD_STD,\n",
    ")\n",
    "\n",
    "pixel_values = np.stack(pixel_values, axis=0)\n",
    "pixel_values = torch.tensor(pixel_values)\n",
    "\n",
    "print(pixel_values.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd5c5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_single_image(image, size=(224, 224), mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]):\n",
    "    # 支持输入：PIL.Image 或 str(路径)\n",
    "    if isinstance(image, str):\n",
    "        image = Image.open(image).convert('RGB')\n",
    "    elif isinstance(image, np.ndarray):\n",
    "        image = Image.fromarray(image).convert('RGB')\n",
    "    elif not isinstance(image, Image.Image):\n",
    "        raise ValueError(\"Unsupported image type\")\n",
    "    \n",
    "    # 调整大小\n",
    "    transform = T.Compose([\n",
    "        T.Resize(size),\n",
    "        T.ToTensor(),  # 转为 [C, H, W] Tensor\n",
    "        T.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "    tensor = transform(image)\n",
    "    \n",
    "    # 添加批维度: [C, H, W] -> [1, C, H, W]\n",
    "    return tensor.unsqueeze(0)\n",
    "\n",
    "# 使用\n",
    "result = preprocess_single_image(image)  # torch.Size([1, 3, 224, 224])\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587d2da2",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550a5ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"describe the image\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c52ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_image_tokens_to_prompt(prefix_prompt, bos_token, image_seq_len, image_token):\n",
    "    #   The input text is tokenized normally.\n",
    "    #   A <bos> token is added at the beginning, and an additional newline token (\\n) is appended.\n",
    "    #   This newline token is an essential part of the input prompt the model was trained with, so adding it explicitly ensures it's always there.\n",
    "    #   The tokenized text is also prefixed with a fixed number of <image> tokens.\n",
    "    #   Unlike in the PaliGemma paper, the Hugging Face code doesn't tokenize \\n separately.\n",
    "    return f\"{image_token * image_seq_len}{bos_token}{prefix_prompt}\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1150525f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d745f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_strings = [\n",
    "            add_image_tokens_to_prompt(\n",
    "                prefix_prompt=prompt,\n",
    "                bos_token=self.tokenizer.bos_token,\n",
    "                image_seq_len=self.image_seq_length,\n",
    "                image_token=self.IMAGE_TOKEN,\n",
    "            )\n",
    "            for prompt in text\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be4f8a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paligemma2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
